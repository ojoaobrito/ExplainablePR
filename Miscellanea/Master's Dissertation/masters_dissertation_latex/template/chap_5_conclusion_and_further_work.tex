\chapter{Conclusions and Further Work}
\label{chap5:conclusion_and_further_work}

The present chapter is a general, high-level overview of the work that was developed in the scope of this dissertation. Therefore, the next paragraphs attempt to validate whether the main goal was achieved, what intuitions were developed and, if possible, how the work could be further improved.\\

Regarding the primary goal of this work (i.e., the adoption of \textit{explainable} principles into a periocular recognition system), the two methods present different, yet equally substantial levels of readability. On one hand, the first method generates images with various shades of green or red, according to each pixel’s contribution to an ”impostor” decision. On the other hand, the second approach conveys its reasoning through text captions that highlight the most different periocular components.

As seen in the results chapter, we managed to deliver images and captions that, more often than not, faithfully explain the differences between two subjects. The first method was able to highlight relevant periocular components such as the irises, eyebrows, skins and even eyelashes. Compared to other implementations, our explanations are definitely easy to read and convey more useful information. Even in terms of the recognition task (i.e., excluding the explainable components), our method attained competitive results to those possible with an existing solution, despite that not being the main focus. As an additional benefit, the recognition stage could be replaced with other, possibly more performant approach, with virtually no cost for the explanations. 

The second method that was proposed focused on the text domain by generating plausible captions for a given "impostor" pair. These text descriptions give emphasis to periocular components that stand out as being too different. In many cases, the captions included the most visually different components, while in others, less obvious (but still valid) components were preferred. 

Based on both methods' results, it is expected that a combination of the two would produce even more complete results, with visual and text based explanations. Such system would take the premise of this dissertation even further.\\

Finally, further development stages of this work could focus on $\mathbf{1}$) reducing the time that it takes to generate "impostor" explanations and $\mathbf{2}$) allowing "genuine" pairs to be explained as well. With regard to the first point, our method is relatively consistent with \ac{LIME} and \ac{SHAP} (while being much better than Saliency Maps), but falls quite behind Huang and Li's method. Consequently, as a way to close this gap, a potential direction in the future could improve the way in which our synthetic dataset is structured. Currently, we mainly divide the images in terms of the position of the irises. This ensures that only a suitable portion of the images ends up being used, avoiding useless calculations. In addition, we could also store the images based on their attributes (i.e., iris colour, eyebrow density, amongst others). By doing so, in inference time, we could have additional processing to determine the attributes of both test images, that make up a pair, and only use the synthetic samples that meet the iris position and attribute constraints, hopefully saving time altogether.

As for explaining "genuine" pairs, a different approach would have to be added, so as to fully cover the possible outputs of our recognition system. Solutions like keypoint matching or image registration could help us understand how little an image $A$ needs to be changed so that it closely matches an image $B$ (which should, more often than not, be the case with "genuine" pairs).

The above improvements (and others that are not described here) could make our work even more valuable, proving that we can take an existing task and make it more transparent for the end user.